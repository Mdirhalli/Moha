---
title: 'HS 614: Final Project'
author: "Mdirhalli"
date: "May 10, 2018"
output: html_document
---


Machine Learning Project on Cervical Cancer Risk Factor dataset


The data collection was collected at 'Hospital Universitario de Caracas' in Caracas, Venezuela. 
The dataset includes demographic information, customs and historical medical records for 858 patients. 
Many patients decided not to answer some questions because of privacy concerns (lost values).

There are 4 target veriables Citology, Hinselmann, Biopsy, and Schiller. In this projec, we
are going to use one target veriable which is Schiller.

The target variables have two classes. 0 = Patient has a negative test result " Patient No Cancer"
                                       1 = patient has a positive test result " Patient Has Cancer"
                                       
                                       
What is Schiller?                                       
Schiller is a medical test in which iodine is applied to the cervix for the diagnosis of cervical cancer.[1]

Schiller's iodine solution is applied to the cervix under direct vision. The mucosa of the normal neck 
contains glycogen and brown spots, while non-natural areas, such as early cervical cancer, do not take
spot. The abnormal areas can then be examined and examined histologically. Schiller's Yoder formula is 
the same as Lugol's iodine composition, and the latter is more concentrated. When iodine is not available from Schiller, iodine can be used in Lugol as an alternative. [1]



Attribute Information:

(int) Age 
(int) Number of sexual partners 
(int) First sexual intercourse (age) 
(int) Num of pregnancies 
(bool) Smokes 
(bool) Smokes (years) 
(bool) Smokes (packs/year) 
(bool) Hormonal Contraceptives 
(int) Hormonal Contraceptives (years) 
(bool) IUD 
(int) IUD (years) 
(bool) STDs 
(int) STDs (number) 
(bool) STDs:condylomatosis 
(bool) STDs:cervical condylomatosis 
(bool) STDs:vaginal condylomatosis 
(bool) STDs:vulvo-perineal condylomatosis 
(bool) STDs:syphilis 
(bool) STDs:pelvic inflammatory disease 
(bool) STDs:genital herpes 
(bool) STDs:molluscum contagiosum 
(bool) STDs:AIDS 
(bool) STDs:HIV 
(bool) STDs:Hepatitis B 
(bool) STDs:HPV 
(int) STDs: Number of diagnosis 
(int) STDs: Time since first diagnosis 
(int) STDs: Time since last diagnosis 
(bool) Dx:Cancer 
(bool) Dx:CIN 
(bool) Dx:HPV 
(bool) Dx 
(bool) Hinselmann: target variable 
(bool) Schiller: target variable 
(bool) Cytology: target variable 
(bool) Biopsy: target variable


Based on this Attribute Information, the variables that are in booleans should be converted to Factors.




Extract Cervical Cancer Risk Factor Dataset. 
Replace the missing values "?" to an empty value.
Note: This information has been provided by the UCI where we got the data from.
They put "?" as a missing values in each value when the patients refused to 
provide these information to them.
```{r}
cervical <- read.csv("C:/Users/kelwa/Desktop/University App/USF/USF Classes/Spring 2018/HS 614/Lab/Lab1/risk_factors_cervical_cancer.csv",
                    na.strings = c('?'),
                    check.names = TRUE,
                        header = T, 
                    stringsAsFactors = FALSE)
```


Building the Models

Now, spilit the data 

Use 'caret' package to split the data into 75:25 by using Schiller feature  
partition the data by using Schiller feature Training, validation and test data

```{r}
library(doParallel)
library(caret)
library(e1071)
library(doSNOW) 
library(ipred)
```



```{r}
set.seed(54321)

indext_traning <- createDataPartition(cervical$Schiller,
                                  p = 0.75, 
                                  list = F)
#print(indext_traning)
```


```{r}
training <- cervical[indext_traning, ]
testing  <- cervical[-indext_traning, ]
```




Data Exploration 


Show complete cases

```{r}
training[complete.cases(training),]
```

Check the data dimension
```{r}
dim(training)
```
Identifying how many NAs are in the data

```{r}
length(which(is.na(training)))
```

Identifying how many observations would we loose, if we removed them?

```{r}
nrow(training)
```

```{r}
nrow(cc_data[is.na(training), ])
```

Use mice package for Multivariate Imputation by Chained Equations - (MICE)

```{r}
library(mice)
```

```{r}
md.pattern(training)

```


Data exploration through visualization.

We are going to use VIM package for visualization and imputation of Missing Values
Visualize the missing values in cervical cancer dataset. This graph also will show us
a combinations of the missing and non-missing values in each colomns.
```{r}
library(VIM)
```

Visualize the missing values in cervical cancer dataset. This graph also will show us
a combinations of the missing and non-missing values in each colomns.
```{r}
aggr(training, prop = F, numbers = T) + labs(title = "NA and Values in Cervical Cancer Risk Factors")

```

To see the previous graph in ratio

```{r}
aggr(training, prop = T, numbers = T)
```


As we see in the grah that the higest column the have missing values in STDs..Time.since.first.diagnosis  
and STDs..Time.since.last.diagnosis.

On the other hand we see the lowest columns that have no missing values are: Age and the target veriables. 


The following graph is showing us that the percentage of each column that has NA values. 
The graph also shows the percentage of the NA and the present values across the cervicalcancer data set
by using "naniar" package.
```{r}
library(naniar)
```

```{r}
vis_miss(training)

```

As we see here, the percentage of missing values are 12.3% and the present values are 87.7%

columns STDs..Time.since.first.diagnosis and STDs..Time.since.last.diagnosis are having
most percentage of the missing values in the dataset.


```{r}
summary(training)
```


After looking at the summary of the training data, we can see there are some features
have no varaince. At this point, we are going to remove them since they are not going
to do any good for out data.

These features are: STDs_Cervical_Condylomatosis "Mean and Max are 0".

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
      0       0       0       0       0       0     105 

      
STDs_AIDS feature has no value. Mean and Max are 0 simillar to STDs_Cervical_Condylomatosis.

   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
      0       0       0       0       0       0     105 
 


Now, we are going to drop off the other 3 target veriables (Citology, Hinselmann, Biopsy) with 

STDs.AIDS, STDs.Cervical.Condylomatosis, STDs..Time.since.first.diagnosis and STDs..Time.since.last.diagnosis
b
 

```{r}
training <- subset(training,select=c(Age, Number.of.sexual.partners, First.sexual.intercourse, Num.of.pregnancies,
                             Smokes, Smokes..years., Hormonal.Contraceptives, Hormonal.Contraceptives..years.,
                             IUD, IUD..years.,STDs, STDs..number., STDs.condylomatosis, 
                             STDs.vaginal.condylomatosis, STDs.vulvo.perineal.condylomatosis, STDs.syphilis,
                             STDs.pelvic.inflammatory.disease, STDs.genital.herpes, STDs.molluscum.contagiosum,
                             STDs.HIV, STDs.Hepatitis.B, STDs.HPV, STDs..Number.of.diagnosis,
                             Dx.Cancer, Dx.CIN, Dx.HPV, Dx, Schiller))
```


Now, let's visualize the data again in a graph 


```{r}
vis_miss(training)

```

Now, we see the percentage of the missing values have dropped down from 12.3% to 8.3% and the present 
values have been increased to 91.7% from 87.7% after we dropped the columns that were having most of
the missing values in the data with the others that had no variance.



We can also see each column that in the cervical cancer dataset via a plot by using Schiller classes
( 0 = No Cancer, 1 = Cancer) since its our target variable in this project.

There are two columns are almost black, which means they have a lot of missing values.  





```{r}
# Set up a data frame indicating missing values by 1
x <- as.data.frame(abs(is.na(training)))
# select columns with some "but not all" missing values
y <- x[,sapply(x, sd) > 0]


# set up a correlation matrix:the missing variables  together have a high correlation
cor(y)
```

Rows are observed variables, and columns are pointer variables for missing values.
The high correlation means that the row variables are closely related to 
the loss of the column variable.
```{r}
cor(training, y, use = "pairwise.complete.obs")
```


Now, we are gonig to convert our target veriable Schiller to Factor to use it on
plot accross all the columns 
```{r}
training$Schiller <- as.factor(training$Schiller)
```


Plot the graph
```{r}
gather(training, x, y, Age:Dx) %>%
  ggplot(aes(x = y, color = Schiller, fill = Schiller)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)
```
This graph gives us a summary of Schiller classes. It's clear to us that Class 0 is 
dominating most of the features. But, if we look at no. 6 (First Sexual Intercourse) 
and no. 25(STDs Syphilis) we see class 1 is dominating the features.

This probably leads us to having an imbalance classification in our training set 
between 0 and 1.


To confirm we have class imbalance, we are going to make a geom bar.

```{r}
library(ggplot2)

```

Check if we have class imbalanced data in our training data
```{r}


ggplot(training, aes(x = Schiller, fill = Schiller)) +
  geom_bar()+ ggtitle("Imbalanced classification?")
```
Yes, we have an imbalanced classification. We will take care of that after we do clustering.






Now, we are going to replace all the NA values in numberics to median and put in a matrix 
for training and testing dataset
```{r}
impute_med <- function(x) replace(x, is.na(x), median(x, na.rm = TRUE))
data_train <- sapply(training, function(x){
    if(is.numeric(x)){
            impute_med(x)
        } else {
            x
        }
    }
)

training <- data_train

```


Convert the matrix to data frame
```{r}
training  <-  as.data.frame(training)
```


Here we are going to convert the catergorcial variables to factors based on IUC. i will assum that 
the same process will be applied on the testing that if I didnt have any. 
```{r}
training$Smokes <- as.factor(training$Smokes)
training$Hormonal.Contraceptives <- as.factor(training$Hormonal.Contraceptives)
training$IUD <- as.factor(training$IUD)
training$STDs <- as.factor(training$STDs)
training$STDs.condylomatosis <- as.factor(training$STDs.condylomatosis)
training$STDs.vaginal.condylomatosis <- as.factor(training$STDs.vaginal.condylomatosis)
training$STDs.vulvo.perineal.condylomatosis <- as.factor(training$STDs.vulvo.perineal.condylomatosis)
training$STDs.syphilis <- as.factor(training$STDs.syphilis)
training$STDs.pelvic.inflammatory.disease <- as.factor(training$STDs.pelvic.inflammatory.disease)
training$STDs.genital.herpes <- as.factor(training$STDs.genital.herpes)
training$STDs.molluscum.contagiosum <- as.factor(training$STDs.molluscum.contagiosum)
training$STDs.HIV <- as.factor(training$STDs.HIV)
training$STDs.Hepatitis.B <- as.factor(training$STDs.Hepatitis.B)
training$STDs.HPV <- as.factor(training$STDs.HPV)
training$Dx.Cancer <- as.factor(training$Dx.CIN)
training$Dx.HPV <- as.factor(training$Dx.HPV)
training$Dx <- as.factor(training$Dx)
training$Dx.CIN <- as.factor(training$Dx.CIN)
training$Schiller <- as.factor(training$Schiller)
```


Convert the missing values in factors to None

```{r}
levels(training$Smokes)<-c(levels(training$Smokes),"None")  #Add the extra level to your factor
training$Smokes[is.na(training$Smokes)] <- "None"           #Change NA to "None"

levels(training$Hormonal.Contraceptives )<-c(levels(training$Hormonal.Contraceptives ),"None")  
training$Hormonal.Contraceptives[is.na(training$Hormonal.Contraceptives )] <- "None"          

levels(training$Smokes)<-c(levels(training$Smokes),"None")  #Add the extra level to your factor
training$Smokes[is.na(training$Smokes)] <- "None"           #Change NA to "None"

levels(training$IUD)<-c(levels(training$IUD),"None")  #Add the extra level to your factor
training$IUD[is.na(training$IUD)] <- "None"           #Change NA to "None"

levels(training$STDs)<-c(levels(training$STDs),"None")  #Add the extra level to your factor
training$STDs[is.na(training$STDs)] <- "None"           #Change NA to "None"

levels(training$STDs.condylomatosis)<-c(levels(training$STDs.condylomatosis),"None")  
training$STDs.condylomatosis[is.na(training$STDs.condylomatosis)] <- "None"       


levels(training$STDs.vaginal.condylomatosis)<-c(levels(training$STDs.vaginal.condylomatosis),"None")  
training$STDs.vaginal.condylomatosis[is.na(training$STDs.vaginal.condylomatosis)] <- "None"         


levels(training$STDs.vulvo.perineal.condylomatosis)<-c(levels(training$STDs.vulvo.perineal.condylomatosis),"None") 
training$STDs.vulvo.perineal.condylomatosis[is.na(training$STDs.vulvo.perineal.condylomatosis)] <- "None"           

levels(training$STDs.syphilis)<-c(levels(training$STDs.syphilis),"None")  #Add the extra level to your factor
training$STDs.syphilis[is.na(training$STDs.syphilis)] <- "None"         

levels(training$STDs.pelvic.inflammatory.disease)<-c(levels(training$STDs.pelvic.inflammatory.disease),"None")  
training$STDs.pelvic.inflammatory.disease[is.na(training$STDs.pelvic.inflammatory.disease)] <- "None"          

levels(training$STDs.genital.herpes)<-c(levels(training$STDs.genital.herpes),"None")  
training$STDs.genital.herpes[is.na(training$STDs.genital.herpes)] <- "None"      

levels(training$STDs.molluscum.contagiosum) <-c(levels(training$STDs.molluscum.contagiosum),"None") 
training$STDs.molluscum.contagiosum[is.na(training$STDs.molluscum.contagiosum)] <- "None"         

levels(training$STDs.HIV)<-c(levels(training$STDs.HIV),"None")  #Add the extra level to your factor
training$STDs.HIV[is.na(training$STDs.HIV)] <- "None"           #Change NA to "None"


levels(training$STDs.Hepatitis.B)<-c(levels(training$STDs.Hepatitis.B),"None")  
training$STDs.Hepatitis.B[is.na(training$STDs.Hepatitis.B)] <- "None"          

levels(training$STDs.HPV)<-c(levels(training$STDs.HPV),"None")  #Add the extra level to your factor
training$STDs.HPV[is.na(training$STDs.HPV)] <- "None"           #Change NA to "None"


levels(training$Dx.Cancer)<-c(levels(training$Dx.Cancer),"None")  #Add the extra level to your factor
training$Dx.Cancer[is.na(training$Dx.Cancer)] <- "None"           #Change NA to "None"

levels(training$Dx.HPV )<-c(levels(training$Dx.HPV ),"None")  #Add the extra level to your factor
training$Dx.HPV [is.na(training$Dx.HPV )] <- "None"           #Change NA to "None"


levels(training$Dx)<-c(levels(training$Dx),"None")  #Add the extra level to your factor
training$Dx[is.na(training$Dx)] <- "None"           #Change NA to "None"

levels(training$Dx.CIN)<-c(levels(training$Dx.CIN),"None")  #Add the extra level to your factor
training$Dx.CIN[is.na(training$Dx.CIN)] <- "None"           #Change NA to "None"




```



Replace all the NA values in numberics to median and put in a matrix for testing data
```{r}
impute_med <- function(x) replace(x, is.na(x), median(x, na.rm = TRUE))
data_test <- sapply(testing, function(x){
  if(is.numeric(x)){
    impute_med(x)
  } else {
    x
  }
}
)

testing <- data_test


```

Convert the matrix to data frame
```{r}
testing  <-  as.data.frame(testing)
```



Here we are going to convert the catergorcial variables to factors based on IUC. i will assum that 
the same process will be applied on the testing that if I didnt have any. 
```{r}
testing$Smokes <- as.factor(testing$Smokes)
testing$Hormonal.Contraceptives <- as.factor(testing$Hormonal.Contraceptives)
testing$IUD <- as.factor(testing$IUD)
testing$STDs <- as.factor(testing$STDs)
testing$STDs.condylomatosis <- as.factor(testing$STDs.condylomatosis)
testing$STDs.vaginal.condylomatosis <- as.factor(testing$STDs.vaginal.condylomatosis)
testing$STDs.vulvo.perineal.condylomatosis <- as.factor(testing$STDs.vulvo.perineal.condylomatosis)
testing$STDs.syphilis <- as.factor(testing$STDs.syphilis)
testing$STDs.pelvic.inflammatory.disease <- as.factor(testing$STDs.pelvic.inflammatory.disease)
testing$STDs.genital.herpes <- as.factor(testing$STDs.genital.herpes)
testing$STDs.molluscum.contagiosum <- as.factor(testing$STDs.molluscum.contagiosum)
testing$STDs.HIV <- as.factor(testing$STDs.HIV)
testing$STDs.Hepatitis.B <- as.factor(testing$STDs.Hepatitis.B)
testing$STDs.HPV <- as.factor(testing$STDs.HPV)
testing$Dx.Cancer <- as.factor(testing$Dx.CIN)
testing$Dx.HPV <- as.factor(testing$Dx.HPV)
testing$Dx <- as.factor(testing$Dx)
testing$Dx.CIN <- as.factor(testing$Dx.CIN)
testing$Schiller <- as.factor(testing$Schiller)
testing$Hinselmann <- as.factor(testing$Hinselmann)
testing$Citology <- as.factor(testing$Citology)
testing$Biopsy <- as.factor(testing$Biopsy)


```

Convert the missing values in factors to 2

```{r}
levels(testing$Smokes)<-c(levels(testing$Smokes),"None")  #Add the extra level to your factor
testing$Smokes[is.na(testing$Smokes)] <- "None"           #Change NA to "None"

levels(testing$Hormonal.Contraceptives )<-c(levels(testing$Hormonal.Contraceptives ),"None")  
testing$Hormonal.Contraceptives[is.na(testing$Hormonal.Contraceptives )] <- "None"          


levels(testing$IUD)<-c(levels(testing$IUD),"None")  #Add the extra level to your factor
testing$IUD[is.na(testing$IUD)] <- "None"           #Change NA to "None"

levels(testing$STDs)<-c(levels(testing$STDs),"None")  #Add the extra level to your factor
testing$STDs[is.na(testing$STDs)] <- "None"           #Change NA to "None"

levels(testing$STDs.condylomatosis)<-c(levels(testing$STDs.condylomatosis),"None")  
testing$STDs.condylomatosis[is.na(testing$STDs.condylomatosis)] <- "None"       


levels(testing$STDs.vaginal.condylomatosis)<-c(levels(testing$STDs.vaginal.condylomatosis),"None")  
testing$STDs.vaginal.condylomatosis[is.na(testing$STDs.vaginal.condylomatosis)] <- "None"         


levels(testing$STDs.vulvo.perineal.condylomatosis)<-c(levels(testing$STDs.vulvo.perineal.condylomatosis),"None") 
testing$STDs.vulvo.perineal.condylomatosis[is.na(testing$STDs.vulvo.perineal.condylomatosis)] <- "None"           

levels(testing$STDs.syphilis)<-c(levels(testing$STDs.syphilis),"None")  
testing$STDs.syphilis[is.na(testing$STDs.syphilis)] <- "None"         

levels(testing$STDs.pelvic.inflammatory.disease)<-c(levels(testing$STDs.pelvic.inflammatory.disease),"None")  
testing$STDs.pelvic.inflammatory.disease[is.na(testing$STDs.pelvic.inflammatory.disease)] <- "None"          

levels(testing$STDs.genital.herpes)<-c(levels(testing$STDs.genital.herpes),"None")  
testing$STDs.genital.herpes[is.na(testing$STDs.genital.herpes)] <- "None"      

levels(testing$STDs.molluscum.contagiosum) <-c(levels(testing$STDs.molluscum.contagiosum),"None") 
testing$STDs.molluscum.contagiosum[is.na(testing$STDs.molluscum.contagiosum)] <- "None"         

levels(testing$STDs.HIV)<-c(levels(testing$STDs.HIV),"None")  
testing$STDs.HIV[is.na(testing$STDs.HIV)] <- "None"           


levels(testing$STDs.Hepatitis.B)<-c(levels(testing$STDs.Hepatitis.B),"None")  
testing$STDs.Hepatitis.B[is.na(testing$STDs.Hepatitis.B)] <- "None"          

levels(testing$STDs.HPV)<-c(levels(testing$STDs.HPV),"None")  
testing$STDs.HPV[is.na(testing$STDs.HPV)] <- "None"           #Change NA to "None"


levels(testing$Dx.Cancer)<-c(levels(testing$Dx.Cancer),"None")  #Add the extra level to your factor
testing$Dx.Cancer[is.na(testing$Dx.Cancer)] <- "None"           #Change NA to "None"

levels(testing$Dx.HPV )<-c(levels(testing$Dx.HPV ),"None")  #Add the extra level to your factor
testing$Dx.HPV [is.na(testing$Dx.HPV )] <- "None"           #Change NA to "None"


levels(testing$Dx)<-c(levels(testing$Dx),"None")  #Add the extra level to your factor
testing$Dx[is.na(testing$Dx)] <- "None"           #Change NA to "None"

levels(testing$Dx.CIN)<-c(levels(testing$Dx.CIN),"None")  #Add the extra level to your factor
testing$Dx.CIN[is.na(testing$Dx.CIN)] <- "None"           #Change NA to "None"

```


Scaling and centering the training and testing datasets by using preProcess function

```{r}
pre_Proces <- preProcess(training, method = c("center", "scale"))

```

```{r}
training <- predict(pre_Proces, training)
```

```{r}
testing <- predict(pre_Proces, testing)

```


Features Selection

We are going to use random forest for selection featurs 
```{r}
library(randomForest)

```


```{r}
rf_fe = randomForest(Schiller~., data= training)
```

Use an importance function based on mean decreasing gini.
Random Forest  referred to as the "average genetic decline", 
represent the importance of each attribute's contribution to homogeneity in data.

```{r}
varImp(rf_fe)

```



Use VarImPlot to plot of importance scores by random forest
```{r}
varImpPlot(rf_fe)
```


From this plot, we are going to take off all the features that are below 1 becasue 
the highest value in our dataset is 10.37 and if we just want to pick up the features
are below the median, we will lose 80% of the data.

```{r}
training <- training[,-c(14,15,16,17,18,19,20,22,23)]

```


Now we have 19 features in our model including the target veriable.


Plot the graph
```{r}
gather(training, x, y, Age:Dx) %>%
  ggplot(aes(x = y, color = Schiller, fill = Schiller)) +
    geom_density(alpha = 0.3) +
    facet_wrap( ~ x, scales = "free", ncol = 3)
```
This a graph to see the remaining features that are in out training data set.




Unsupervised Learning 


Hierarchical Clustering



Hierarchy is an alternative approach that builds a hierarchy from bottom to top,
and does not require us to quantify the number of clusters in advance.


Why Hierarchical Clustering?

It allows us to avoid this annoying problem of early cluster repair.
Also, the hierarchical Clustering is that the dendrogram,captured as 
a result of a hierarchical cluster, can allow us to visualize cluster 
clusters very accurately without having to restart the algorithm.

Furthermore, hierarchical clustering usually allows us to specify
any distance scale we want to specify the spacing between points.


How Hierarchical Clustering works?

1. Place each data point in its cluster.
2. Identify the closest two groups and merge them into one group.
3. Repeat the above step until all data points are in a single block.
4. Once done, it is usually represented by a symbolic program such as structure.




Normalization

We can normalize a variable by subtracting mean and dividing by standard deviation
Note, to do clusting analysis, we need to have quantitative data


There are some variables that are very low as value 0

But as variable as age, smoke years have higher values, so we need to 
set data standards so that all variables have a level field.
This should not happen only because some values are too high. They will
dominate the show of incitement. Thus, we form assemblies, do not do
You want one variable to control just because the notes are on top
side. When we normalize all the variables, the mean of each variable
becomes zero and the standard deviation is approximately one. 
Thus, this creates a level playing field.


Make a new data that we can use it for normalization becasue we don't
want to harm the orginal data for another clustring alaysis


So, we need to delete Schiller feature because Normalization only deals with 
quantitative variable. We are going to sing a new data called.

Befor we go further, we can't do normalization on our
data becasue we have too many categorical features and
that would not let us to get the mean and the standard diviation
when we scale the data set. 


Explore the dataset again
```{r}
str(training)
```


let's Scatterplot for Number.of.Sexual.Partners and Dx.HPV test to explore 
our clustering data.

```{r}
plot(training$Number.of.sexual.partners ~ training$Dx.HPV)

```


From this graph, we can see that we have three different groups. The first group is 0, 
the second one is 1, and the third one is None "Which has no value". In a group 0, 
we see most of the patients have 0 sexual partners. But when we look at the outliers,
there are 6 patients who have less than 5 sexual partners had done HPV and the result
is negative. Also, we see there is a patient who has around 15 sexual partners and 
the patient's test result is negative. That's probably the patient's sexual partners 
were using safe sex.


On the other hand, we can see most patients who have less than 1 sexual partner have 
HPV positive. And we see some outlier patients who have less 2 sexual partents and their
HPV is positive. That's probaply the sexual partents for the patiets were using safe sex.


We can see that we have 2 clusters, but these two clusters are based only on two variables:
Number.of.Sexual.Partners and Dx.HPV.


Now, we are going to  calculate the euclidean distance or euclidean matric.
We  use dist function to calculate euclidean distance and name it distan

```{r}
distan <- dist(training)
distan
```
This gives us the lydian distance among all different patient records. We have 644 rows in
the dataset. This tells us how close or how far each patient is compared to others. We can see
here the the only variable that has a distance is the first one with a value 2.68439225. This means
the first row in that patient and the second patient they are dissimilar in terms of
thoes variables because the distance is quite low.

That gives us a sence the patients are quite close to each other in cluster one.
That might give us a sence of having one cluster.


Now, we cluster Dendrogram with Complete Linkage 
```{r}
hc_c <- hclust(distan, method = "complete")
```

Let's Plot the Complete Linkage
```{r}
plot(hc_c, lables = train_data$Schiller)
```


From this Clustering Hierarchical shart, it's hardly to read the observations below due to 
the overlaping in the data set to determine how many clusters we have. 
But, we can see there are more than 2 cluster groups that belong to different categories.


Let's try using the average Linkage for clustering by using method Average
```{r}
hc_a <- hclust(distan, method = 'average')
```

Let's plot it and see if we will have a different observation than the previous one.
```{r}
plot(hc_a)
```

Obviously, the formating of clusters are slightly different and we can see
that there is still overlap in both plots, which makes it hardly to read 
the clusters in the bottom.


Let's try using the single Linkage for clustering by using method Single

```{r}
hc_s <- hclust(distan, method = 'single')

```
plot
```{r}
plot(hc_s)
```

As we see on the Hc single is the worest among the other linkages and you cant read the observation 
below due to the overlap.


As we see from the plot that the best choices for total number of clusters are either 2 or 3


We are going to use cutree to cut off for the tree in completer and average linkages based 
on our prefrence nubmer of cluster which is 3.

```{r}
cut_a <- cutree(hc_a, 3)

```

```{r}
cut_c <- cutree(hc_c, 3)

```

Now, we are going to make a table to compare the cut tree of compelete linkage and average linkage too.
```{r}
table(cut_c, cut_a)
```

This table shows us that using average method, there were 638
patients belong to cluster 1.
2 patients belong to cluster 2.
and 1 patients belong to cluster 3.


In complete linkage method, there were 638 patients   
belong to cluster number 1.
5 patients belong to cluster number 2.
3 patients belong to cluster number 3.

Now, lets compare patients who are in avrage linkage versus 
patients who are in compelte linkage. We see there were very good match
for 638 patients. Both linkage methods listed them as cluster 1.
similarly, there were a good match for 5 patients in both linakage methods
were listed them in cluster 2. And there were a match in cluster no. 3 in
both linkage methods were they match for 1 patient,


This table allows us to compare two different methods and see how cluster 
formation is behaving, we use avrage linkage versus complete linkage.




Cluster mean

Now, we are going to calculate the mean of the complete cluster
by using aggregate function

```{r}
cm <- aggregate(train_data, list(cut_c), mean)
cm
```


We see here that we get average values for the 3 clusters for 
each variable. So, this will help us  characterizing these 3 clusters.
If we dont see too much variation among these 3 averages for a variable,
that means that varaible is not really palying a very significant role 
in deciding cluster patient for the Schiller's classes.

If we look at the outcome, we see Age has the highest value 0.870960811.
On the other hand, we see there are a lot of negative values in most features. 
Clearly, Age seem it has a significant impact on clusterd patients. 
Thus, this means that patiens who belong to cluster 2 they have higher Age.

On the other hand, we see patients who belong to cluster 1 and 3 have lower
score than average.



Let's do Scree plot. Scree plot will require cacluation of within
gorup sum of squares by usin "cluster" library so within group sum of of squares.

```{r}
library(cluster)
```



The reason why I chose to do Scree plot is that I tried to do Silhouetter plot
where I can represent the complete hc linkage for the 2 and 3 clusters 
that I did earlier with the distance. But, we I run it, it did not show me
anything due to either to high number of patients we have in the traning set or
the overlapping.

```{r}

within_sum_s <- (nrow(train_data)-1) * sum(apply(train_data, 2, var))

for (i in 2:20) within_sum_s[i] <- sum(kmeans(train_data, centers = i)$withinss)
plot(1:20, within_sum_s, type = 'b', main = "Scree plot",
     xlab = "Num of Clusters", 
     ylab = "Within Group Sum of Squares")


```


This plot gives me an overview of all possible clusters and within
group sum of squares. Within group means within cluster variability,
we want to reduce within cluster variability. So, when we go from one
cluster to two clusters you can see the drop in within group sum of squares
is very large. But, we try to see is from cluster no. 7 and more clusters, 
the improvement somehow is not significant. The drop in variability or within
sum of squares is not that much.

The screen segment in this case indicates that the number should be less than 
the clusters. Falling 2 or 3, far from that gains are not too big. Any distance
scale we want to separate the spaces between points.






K-Means Clustering.


What is K-Means clustering?

K-Means Clustering is an unsupervised learning algorithm that tries 
to collect data based on its similarity. Unattended learning means 
that there is no predictable result, and the algorithm only tries to 
find patterns in the data. In k-means grouping/clustering, we have to 
determine the number of groups/clusters in which we want to collect data. 
The algorithm specifies each random observation for each group, and finds 
the center point for each group.


Why k-Means Clustering?

Easy to implement with a large number of variables,
K-Means may be faster than hierarchical clustering "if K is small".
K-Means may produce higher clusters than hierarchical clusters.
The example can change the group (move to another group) when 
recalculating the center points



in the begining we need to know how many clusters we are interested in by
using the training data set. let's use the same clusters that we used in 
Hierarchical clustering, which are 2.

use k-means function by storing it in kcls

```{r}
kcls <- kmeans(training, 3)
```

Let's see what this kind of out put is giving us
```{r}
kcls
```

it seems like we got a lot of information out of this. 
the first line is of K-means clustering with 3 clusters of sizes 30, 240, 374.
so, first cluster has 30 patients in it. Second cluster has 240 patients in it 
and the third cluster has 374 patients. 

The second line is Cluster means.

The third line is the Clustering vector which means where each 
patient should go into their clusters. 

For example, the first patient should go to the cluster no. 3 and
the third cluster should go to cluster no. 2.

We also have Within cluster of squares by cluster.

 in
The sum of squares within cluster variability is 429.6599 for 
the fisrt cluster, whereas cluster 3 is 1455.6791, which has 374 patients.
this means the variability is higher that means the patients are far from
each other in terms of distance.

The lower sum of squares within cluster variability is 429.6599, which
goes to cluster no. 1 with 30 patients. This means the patients are close 
to each other in terms of distance, which has more compact than the second 
and the third cluster.

The 22.5 %% is the aggregate variance meter in our the data set that 
is explained by the clustering.

 (between_SS / total_SS =  22.5 % %)

k-means reducing dispersion within the group and increasing dispersion 
between groups. By assigning samples to clusters k rather than number 
of samples, reduction clusters were achieved in groups of squares of 25.3%.

There are various componenets of this cluster analysis that is available.
for example, if we want to look at cluster one, we can say kcls$cluster 


```{r}
kcls$cluster
```

From this, we can see it gives us the cluster of the patients that is 
on the above out put.


Let's plot two features on scatt by looking at the cluster means
to chose from.

We are going to use Number.of.sexual.partners with Dx.HPV test


```{r}
plot(training$Number.of.sexual.partners ~ training$Dx.HPV, training, 
     col = kcls$cluster)
```

From this graph, we can see that we have three different clusters The first group 
is 0, the second one is 1 and None, which has no values. In a group 0, we see most 
of the patientshave non or 1 sexual partners.But when we look at the outliers, 
there are 6 patients who have less than 5 sexual partners had done HPV and the result 
is negative. Also, we see there is a patient who has over 6 sexual partners and the - patient's test result is negative. That's probably the patient's sexual partners were 
using safe sex.

On the other hand, we can see most patients who have less than 3 sexual partners have 
Cancer positive. That's probaply the sexual partents for the patiets were using safe sex.


We can see that we have 3 clusters, but these two clusters are based only
on two variables:
Number.of.Sexual.Partners and Dx.HPV.


Clustering is good when between cluster distance is high and
within cluster distance is low. But in this case, all 3 clusters
variables are not that good except the one in black. 



However, we might get a different result if we plot all the 3 clusters with their 
observations in a graph without specify the column like we did in the previous plot.

We are goint to use clustplot function from clustplot library


```{r}
library(cluster)

clusplot(training, kcls$cluster, main = "2D represntation of cluster",
          color=TRUE, shade=TRUE, 
         labels=2, lines=0)
```

In this plot, it's obviously that all the patients have a lot of overlap 
among the three clusters.

Clustering is good when between cluster distance is high and within cluster
distance is low. In this case, we didnt get a good separation between the clusters.




Classification / Supervised Learning



For classification, I am going to use Random Forest through
caret algorithm because caret provides different algorithms in one.
That would make the work easy to do by working in one algorithm.

Due to noise characteristics in the sensor data, the random forest model is best for this task.
The first model (model1) achieves an error rate of 1.58%  with 88.3 % accuracy rate. also, 
the gap between the Sensitivity : 0.45455 is not as high as xgbtree model.         
                    Specificity : 0.88542  
                    
But,the error rate in gbtree was lower than Random Forest  1.16%, but the gap between
Sensitivity : 0.00000 and Specificity: 0.98438 is very high, which is not a good to predict
for our dataset
                 
xgbtree takes mouch longer than random forest to predict. 
Random forest works with small and large datset. 

We are goning sign it up with model.rf and compain 
Train control and RF model in one model.



```{r}
set.seed(54321)
model.rf <- caret::train(Schiller ~ .,
                         data = training,
                         method = "rf",
                        trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 3, 
                                                  verboseIter = FALSE))
```



```{r}
cm.riginal <- confusionMatrix(predict(model.rf, 
                                    testing), 
                               testing$Schiller,
                               # Since our traget Schiller's
                               # calss is 1 = Patient has cancer 
                               positive = "1")

cm.riginal
```
We see here reference, which is the acctual values
and prediction is based on the prediction model that we
just build.

There are TN = 192 patients who did Shchiller test, but the result is 
negative. both Refrence and Prediction agree on that there are 
196 patients who have no cancer.

On the other hand, both Refrence and Prediction agree that
there is TP = 0 non-patient who did the schiller test
and get a positive resutl. This is a correct prediction. 
When we add the correct prediction to gether TN + TP (192 + 0) we get
192 and we divide it by 214 which is our test data(215) we get the -
accurace for the model which is 192/ 214 = 0.8971963.

This model is accurate about 89% at the time. Also, it fills between 
(0.8485, 0.9344) of 95% of Confidence interval.


If we look at "No Information Rate" which is the same
proportion of the observed calss, we find out that it's 0.8972
which is not higher or lower than our model accuracy 0.8972.
That means this is a good model.

Let's look at the Sensitivity and Specificity of the model.
Sensitivity formula  => TP/TP+FP => 0/(0+192) = 0
Specificity formula  => TN/TN+FN => 196/(196+0) = 1


As we see, the Specificity for this model is very hight
and the Sensitivity is very low 0. There reason why we have
a huge gap between Sensitivity and Specificity is that 
we had few points for class 1, but much more for class 0. 
Thus, this model is domenated by class 0 and doing a good 
job in Specificity, but not so good for predicting class 
1 where Sensitivity is 0.


Since we are interested in prediction class 1, this model
is too bad because the Sensitivity is too low 0.


What we can do to improve this situation is by doing
undersampling for better Sensitivity.


Build Under-sampling model by using Random Forest 
Through Caret


Re-sampling

Build Under-sampling model by using Random Forest 
Through Caret to fix the imbalance classification

The reason why we are going to use Random Forest for resampling
because Random Forest provides an easy and not complicated way with different type of techniques
to fix this issue such as undersampling, oversampling, ROSE, and Both.

Fortunately, the caret makes it very easy to incorporate over-sampling and sampling 
techniques with the re-diagnostic process through verification. We can simply add
the sample sampling option to our control element, and choose Down to take samples 
under. The rest is the same as with our original model.


```{r}
under_tcr <- trainControl(method = "repeatedcv", 
                           number = 10, 
                           repeats = 15, 
                           verboseIter = FALSE,
                           sampling = "down")

```

We are going to use 500 ntree to get a high senesitifity since our target class is 1.

Note, i tried different ntree numbers and this was the highest sensitifity I could get with
a good accuracy for the model.

```{r}
set.seed(54321)
under.model.rf <- caret::train(Schiller ~ .,
                         data = training,
                         ntree = 500,
                         method = "rf",
                         trControl = under_tcr)
```




```{r}
cm_under <- confusionMatrix(predict(under.model.rf, testing), 
                               testing$Schiller,
                               # Since our traget Schiller's
                               # calss is 1 = Patient has cancer 
                               positive = "1")

cm_under
```
We see here Reference, which is the acctual values
and Prediction is based on the prediction model that we
just build by using under-sampling.

There are TN = 170 patients who did Shchiller test, but the result is 
negative. both Refrence and Prediction agree on that there are 
170 patients who have no cancer.

On the other hand, both Refrence and Prediction agree that
there are TP = 10 patients who did the schiller test
and they get positive resutls "They have cancer". 
This is a correct prediction. 

Accuracy:

When we add the correct prediction to gether TN + TP => 170 + 10 => we get
180. And if we divide 180 by 214 which is our test data "214" we get the -
Accuracy for the model which is 180/ 214 = 0.8411215.

Accuracy  = TN+TP/Total of Test Observations
          = 170 + 10/214
          = 0.8411215

This model is accurate about 84.1% at the time. Also, it fills between 
(0.7851, 0.8874) of 95% of Confidence interval.


If we look at "No Information Rate" which is the largest
proportion of the observed calss, we find out that it's 0.8972
which is a bit higher than our model accuracy 0.8411215.
That means this is somehow relevent model. But, we still need to consider 
the Sensitivity.

Let's look at the Sensitivity and Specificity of the model.
Sensitivity formula  => TP/TP+FP => 10/(10+12) = 0.4545455
Specificity formula  => TN/TN+FN => 170/(170+22) = 0.8854167

As we see, the Sensitivity for this model is lower than our mode 0.8411215
and the Specificity is higher than the Sensitivity 0.8854167 but, the gap is 
not as high as the first model we built.


Thus is still not a good for predicting class 1 where Sensitivity is 0.4545455,
but this the best we got in comparing with the the first model we built.



The p-value tells us how likely it is to get a result like this if the null 
Hypothesis is true.

The singnificant level for P-value is a = 0.05

Our P-value is 0.9960 which means it is higher than > 0.05
so we accept the Null Hypotheses (Null Hypotheses is true)
Now, let's see the confusion Matrix in a grah (TP, TN, FN, and FP)
```{r}
results <- data.frame(actual = testing$Schiller,
                      predict(under.model.rf, testing, type = "prob"))
results$prediction <- ifelse(results$X0 > 0.5, "0",
                             ifelse(results$X1 > 0.5, "1", NA))

results$correct <- ifelse(results$actual == results$prediction, TRUE, FALSE)

ggplot(results, aes(x = prediction, fill = correct)) +
  geom_bar(position = "dodge")+  ggtitle("confusion Matrix for Under Sampling model")
```

This graph shows us that the different between T and P in the confusion matrix.

TN = 170    FP = 12
FN= 22      TP = 10



Xgb Model 

```{r}
set.seed(54321)
model_xgb <- caret::train(Schiller ~ .,
                          data = training,
                          method = "xgbTree",
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 10, 
                                                  repeats = 10, 
                                                  savePredictions = TRUE, 
                                                  verboseIter = FALSE))
```


```{r}
cm.xgb <- confusionMatrix(predict(model_xgb, 
                                    testing), 
                               testing$Schiller,
                               # Since our traget Schiller's
                               # calss is 1 = Patient has cancer 
                               positive = "1")

cm.xgb
```





```{r}
tab_xgb_pred <- predict(model_xgb, testing)


```


Now, we use Misclassification Rate by using 
predict function and store it in pred_under.
```{r}
tab.xgb.pred <- table(tab_xgb_pred, testing$Schiller)
tab.xgb.pred
```
Error rate in xbg model

```{r}
1- sum(diag(tab.xgb.pred))/sum(tab.xgb.pred)

```
The reason why did I chose Random Forest over SVM is that
Random Forest is essentially suitable for multi-layer problems, while SVM is essentially two rows. 
For a multilayer problem, you willl need to reduce it to multiple binary classification problems.

For the classification problem, Random Forest gives you the possibility of belonging to the category. SVM gives you the distance to the limit, still need to convert it to probability in some way if you need probability.

Random forests do not take too long to train, especially if you do it in parrallel, something no one can do with SVM or enhanced trees.

SVM gives you a support guides which are points at each score closer to the line between categories. It may be in their own interest to interpret.
SVM usually needs grid-search in hyperparameters
SVM slow and heavy on the memory.

The raason why I didnt chose GLM is that it can suffer significant inaccurate losses 
if you do not add the variables/terms of the correct commands and the correct interaction
conditions. So you have to draw/preview your data distribution before thinking about
adding any variable/term to your form.

Therefore, before GLM follows, it is essential that your data set meets the scientific assumptions.

Random Forest uses for classification and regresission.

However, Random Forest has some drowback such as:
Inconsistency. 
Difficulty for adaptation.
Computationally intensive.
Not the quickest classifier, but plenty fast in practice.
lastlty, it's memory hungry if you are working on a laptop that has not high memory.


```{r}
sum(diag(tab.pred))/sum(tab.pred)
```


Now, we use Misclassification Rate by using 
predict function and store it in pred_under 
to compare our model with logistic regression 

First, we use predict function to predict our model and test data 
```{r}
pred.under <- predict(under.model.rf, 
                      testing)
head(pred.under)
```

Now, we use Misclassification Rate by using 
predict function and store it in pred_under.
```{r}
tab.pred <- table(pred.under, testing$Schiller)
tab.pred
```

```{r}
sum(diag(tab.pred))/sum(tab.pred)
```
From this calculation we can see we have correct classification 
in our model.

However, if we want to see the missclassification rate is 
by using the same calculation that we did earlier with
1 - in the front of sum

Error rate in Random forest under samplying model 
```{r}
1- sum(diag(tab.pred))/sum(tab.pred)

```

The missclassifcation rate is 0.1588785
and if we look back to the classification result we still 
have a good classifcation rate with 0.8411215.

Now, we want to get the F1 score of the prediction
by using accuracy.meas function from ROSE library and use 
our test data signed by Schiller as a response
, pred_under as predicted , and threshold = 0.5 because
our class threshold is positive 1 = patients have cancer.

F1 score used to test our classifier accuracy



```{r}
library(ROSE)
```

```{r}
accuracy.meas(testing$Schiller, pred.under, threshold = 0.5) 
```

Since we know that the worest value for F1 score is 0 the best value is  1 
to chose for the model. From this result, we can see that the F1 score is barley
above 0 "0.093" and less than one, but still a good model.




Reciever Operation Characteristic (ROC) Curve

First, we want to compare our prediction model with test data 
by using head function
```{r}

head(pred.under)
```

```{r}
head(training$Schiller)
```



```{r}
head(testing$Schiller)
```
If we compared the first predicton patient that we did through 
pred.under with the first probability patient who did Schiller
test in the training data we will see both of the patients have 
calss 0 "no cancer".

However, if we looked at the third prediction patient and compare it
with the scond probability patient we see the prediction patient has
calss 1 "patient has cancer", but the probability patient on the traning data
has class 0 "has cancer" so this is a classification error.


Let's put this in histogram and see the probability of the prediction model

```{r}
histogram(pred.under)
```
As we see here the probabilites are either 0 "Patient has no cancer" 
 or 1 "patient has cancer". If we one type of classification 0 or 1 we will 
 have one type of classification, but if we use something in between 
like 0.5, the miss classification or accuracy might change when we plot the graph.


now, we are going to use prediction function with our OCR
Since our target variable is factor, we need to change it to numeric otherwise
prediction function will not work
```{r}
pre <- prediction(as.numeric(pred.under), as.numeric(testing$Schiller))
pre
```

Note, we see here the values of our schiller's class has changed

0 "Patient has no cancer" becomes => 1
1 "patient has cancer"  becomes => 2


Now, we are going to use performance function to evaluate our model
and sing in evaluate and then plot it.

```{r}
library(AUC)
```



```{r}
evaluate <- performance(pre, measure = "tpr", x.measure = "fpr")
# plot the ROC curve with color and tittle
plot(evaluate, colorize = T,
     main = "ROC Curve",
     # Change the False Positive Rate to Sensitivity
     ylab = "Sensitivity",
     # Change True Positive Rete to 1 - Specificity which 
     # is the actual value in our model
     xlab = "1 - Specificity")
# Add the stright line
abline(a=0, b=1)
# Now we caclualte the area under the curve
auc <- performance(pre, measure = "auc")
auc <- auc@y.values[[1]]
# its 0.6377551
# Lets round it to 4 decimal
auc <- round(auc, 4)
# Add it to the graph 
legend(.6, .2, auc, title ="AUC")

```

The cut value is around 0.1 in Fales positive rate. But, we see the cut off
on the True positive rate is 0.5 so the accuracy level is good. 
In fact, it's close to 50% and it rapidly rises as we increase the cut off 
values and reached 1.0 point. 

Also, we see the curve starts somewhere around 0 on the x side
and goes to 1.0, and then this value which is 3 as we see in the high right corner.
That would have classified in a perfect way or accuracy would have been 100%.
But in reality, based on the data we get this curve which is not really close to
the ideal value.

The straight line here means without any model if we say out of 644 patients get 
a negative result will be right about 65%. If the model does worst than that 
the curve would be down the line. In this case, our model is doing better.

When we look at the color line on the right side, we will see the light green color 
is where the cut-off. Based on the cut off values range from point 2.5 up to 3.



Calculate Area Under Curve (AUC)


```{r}

auc <- performance(pre, measure = "auc")
auc <- auc@y.values[[1]]
auc
legend(.6, .2, auc, title ="AUC")
```

To confirm our calculation, we use unlist function.
```{r}
auc_c <- performance(pre, "auc")
auc_c <- unlist(slot(auc_c, "y.values"))
auc_c
```


References:

[1]. Wikipedia. Schiller's test.[Internet]. 2018[cited 2018 May 05]. Available from:  https://en.wikipedia.org/wiki/Schiller%27s_test#cite_note-1
